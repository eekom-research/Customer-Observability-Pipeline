{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8e173-5880-4be7-bc60-2686cddfca42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta,timezone\n",
    "from db import Model, Session, engine\n",
    "from models import Tweet, Company\n",
    "from sqlalchemy import select\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re #regular expression\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis, pyLDAvis.lda_model\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302074d3-3943-4c39-91ff-cf21025dc294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7022f-0ff1-4d54-9285-ec734bc2764b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb6ecf-5eef-4b10-a11b-d93ad5a0661d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4899b1f-1e85-4a23-8b15-656b6bd68a3a",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158aca4-3790-4116-b286-475d5c01f124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = select(Tweet).where(Tweet.company_id == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505d117-ffbb-47e9-a61e-f41594c6b1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49807667-dc59-49a1-a51d-6dc3073aaf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995843c-e26f-4cb2-9dff-e0471a530e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test query output\n",
    "with Session() as session:\n",
    "    result = session.scalars(query).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0c043-cbe6-4d68-884a-40691a15ef80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0090b13-1258-4395-bb66-8a4bf873ba19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result[0].date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee95cc-4e96-47ab-8ca2-b46050eb66bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define emoji removal helper function\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text,replace=\"\")\n",
    "\n",
    "def decode_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \"\"))  # Removes colons from the description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee743e2b-caba-4fc9-bbad-74a7d2742f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9c958-c794-4b66-94c3-c2a2dfbd7d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_id = [tweet.id for tweet in result]\n",
    "tweet_text = [tweet.text for tweet in result]\n",
    "tweet_date = [tweet.date.astimezone(timezone.utc) for tweet in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8ffae-9e8f-442f-a643-fce9c0f258b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame({'id': tweet_id,\n",
    "                         'text':tweet_text,\n",
    "                         'date':tweet_date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f2277-e5d9-4410-9a9c-68c0d8b0bce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c06b0-e916-4af1-a03d-2d866e25613b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df['text'] = tweets_df['text'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573e418-8f38-4fa3-accb-4be3ec273d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4ea7e-5d55-4aff-b868-e917cd41a2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd39f71-88b9-4878-8fdb-35676056581f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = tweets_df.loc[tweets_df['text'].str.contains('@gtbank', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81304cce-cc6b-47d5-a512-affbe65013c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4686d-37c5-4fcc-b1c4-95cc451baefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start preprocessing and pipeline creation\n",
    "class TextPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        processed_texts = []\n",
    "        for text in X:\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "            processed_texts.append(' '.join(lemmatized_tokens))\n",
    "        return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb955ec0-b8b8-4e8e-8f33-1ff32597b2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vect', CountVectorizer(stop_words='english'))])  # Custom preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e18c9-264e-45b9-be76-d43ac6e55810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_tweets = preprocessing_pipeline.fit_transform(filtered_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d1a96-269d-4a78-9323-d176081565a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(processed_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc541f8-b180-4682-9acc-c1b9fe053d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_tweets[0].todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d6d7c-f49c-44d7-abd7-e133632d1b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248a76d-4cf8-4b26-80fc-e980dd38bd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def normalize_text(documents,\n",
    "                   min_token_len=1,\n",
    "                   irrelevant_pos=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Keyword arguments:\n",
    "    documents -- (np.array[str]) the list of documents to be preprocessed\n",
    "    min_token_len -- (int) min_token_length required\n",
    "    irrelevant_pos -- (list) a list of irrelevant pos tags\n",
    "\n",
    "    Returns: np.array[str] the normalized documents\n",
    "    \"\"\"\n",
    "    normalized_documents = []\n",
    "\n",
    "    for text in documents:\n",
    "        #print(text)\n",
    "        # Remove Emails\n",
    "        text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "        # Remove extra space characters\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Remove distracting characters\n",
    "        text = re.sub(r'''[\\*\\~]+''', \"\", text)\n",
    "\n",
    "        doc = nlp(text) #covert text into spacy object\n",
    "        clean_text = []\n",
    "\n",
    "        for token in doc:\n",
    "            if (token.is_stop == False # Check if it's not a stopword\n",
    "                and token.is_alpha # Check if it's an alphanumerics char\n",
    "                and len(token) > min_token_len # Check if the word meets minimum threshold\n",
    "                and token.pos_ not in irrelevant_pos): # Check if the POS is in the acceptable POS tags\n",
    "                lemma = token.lemma_ # Take the lemma of the word\n",
    "                clean_text.append(lemma)\n",
    "\n",
    "        clean_text = ' '.join(clean_text) #merge list of tokens back into string\n",
    "        normalized_documents.append(clean_text) #append to list of normalized documents\n",
    "\n",
    "    normalized_documents = np.array(normalized_documents) #convert list of normalized documents into numpy array\n",
    "    return normalized_documents\n",
    "\n",
    "# Create a Transformer from the function so that we can use it in a Pipeline\n",
    "normalizer = FunctionTransformer(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09785666-4e06-440e-bd9c-798863bc22f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_str = filtered_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d2482-758d-4d6a-9c41-d3eebe18cb0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Prior to normalization:\\n{test_str}')\n",
    "print(f'After normalization:\\n{normalizer.transform([test_str,])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f7e9b-e49f-4b29-ab75-ea0a846666d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#grouped_df.info()\n",
    "filtered_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61c59c-173f-4775-8207-91bce2d48522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create weekly bins and group by these bins\n",
    "# filtered_df['datetime'] = pd.to_datetime(filtered_df['date'])\n",
    "# filtered_df['weekly_bins'] = filtered_df['date'].dt.to_period('W')\n",
    "# grouped_df = filtered_df.groupby('weekly_bins')['text'].agg(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107ce26-8545-46c6-83f4-af48fcb6c9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the built-in stopword list\n",
    "builtin_stopwords = list(ENGLISH_STOP_WORDS)\n",
    "# Your custom stopwords\n",
    "custom_stopwords = ['una', 'dey', 'come','dm','pls','guy',\n",
    "                    'hi','try','hello','god','gtb','gtbank','nigeria','till','gt',\n",
    "                   'send','month','week','day','february','don','useless','want',\n",
    "                    'people','know','abeg']\n",
    "# Combine the stopword lists\n",
    "all_stopwords = builtin_stopwords + custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096e5f0-ea4a-4094-9841-9aac6dc6d8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d08a7-e4dd-4135-9afb-2f837cb956fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_features = 5000\n",
    "\n",
    "#keep 5000 most common tokens that appear in atleast 2 documents, less than 95% of documents\n",
    "#notice binary=False by default\n",
    "vectorizer = CountVectorizer(min_df=50, max_df=0.95, \n",
    "                             max_features=n_features,\n",
    "                             ngram_range=(1, 3),\n",
    "                             stop_words=all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df3ad0-cb89-4107-864b-df890b114fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([('normalizer', normalizer),\n",
    "                         ('vectorizer', vectorizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059998b7-c49f-437f-b755-676a0adfdaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_topics = 15\n",
    "\n",
    "#alpha = doc_topic_prior = 1 / n_components (every topic is equally likely in a document)\n",
    "#eta = topic_word_prior = 1 / n_components (every word is equally likely in a topic)\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                max_iter=10,\n",
    "                                doc_topic_prior = 0.01,\n",
    "                                topic_word_prior = 0.91,\n",
    "                                learning_method='batch',\n",
    "                                random_state=27)\n",
    "\n",
    "pipeline = Pipeline([('preprocessor', preprocessor),\n",
    "                     ('model', lda)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be060a2-67e0-4c68-b594-c711bc06d462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.fit(filtered_df['text'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc031dfa-7fa7-4970-84cf-01a356598349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df['text'].iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadebfeb-92ee-4327-a233-c307e7d839e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[print(max(range(len(topic)), key=topic.__getitem__)) for topic in pipeline.transform(filtered_df['text'].iloc[0:10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e81a8-3a45-4e0f-9d11-de63a9a28b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a4528-bdad-4940-83c9-7b5316266918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fd1af-e8c9-4be2-928d-79875995305e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "plot_top_words(lda, vectorizer.get_feature_names_out(), 10, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9f81f-4dc3-4001-8623-1e7bfb3c0aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_vectorized = preprocessor.transform(filtered_df['text'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920c616-a4c4-49ca-b571-9e3ca96d75a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis = pyLDAvis.lda_model.prepare(lda, data_vectorized, vectorizer, mds='pcoa',sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acf202-8613-4374-9907-05e442fd5c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e2b1f-3bd1-4830-8a23-4aad394f255d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer_pipeline = Pipeline([('normalizer', normalizer)])\n",
    "data_normalized = normalizer_pipeline.fit_transform(filtered_df['text'])\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in data_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d05b5-f714-4d5b-bd24-a90750d3cc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48801da-cc73-4ce3-9195-1bb84e9731f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5000\n",
    "\n",
    "#keep 5000 most common tokens that appear in atleast 2 documents, less than 95% of documents\n",
    "#notice binary=False by default\n",
    "vectorizer = CountVectorizer(min_df=2, max_df=0.75, max_features=n_features)\n",
    "\n",
    "preprocessor = Pipeline([('normalizer', normalizer),\n",
    "                         ('vectorizer', vectorizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544b63a-db82-41ca-94ad-87f81150e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume 'pipeline' is your sklearn pipeline ending with CountVectorizer\n",
    "processed_corpus = preprocessor.fit_transform(filtered_df['text'])\n",
    "\n",
    "# Convert sklearn's document-term matrix to Gensim's corpus format\n",
    "gensim_corpus = Sparse2Corpus(processed_corpus, documents_columns=False)\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "gensim_dictionary = Dictionary.from_corpus(gensim_corpus, id2word=dict((id, word) \n",
    "                                        for word, id in preprocessor.named_steps['vectorizer'].vocabulary_.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c659c5c-5d3c-48d6-8a75-dda129eb8a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, you can create a Gensim LDA model with this corpus and dictionary\n",
    "lda_model = LdaModel(corpus=gensim_corpus, \n",
    "                     id2word=gensim_dictionary,\n",
    "                     random_state=100,\n",
    "                     alpha = 'asymmetric',\n",
    "                     eta = 0.91,\n",
    "                     num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb52cae-3005-4e04-8dda-6ae058b5bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CoherenceModel using the LDA model, the tokenized documents, and the dictionary\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=tokenized_docs, dictionary=gensim_dictionary, coherence='c_v')\n",
    "\n",
    "# Get the coherence score\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# Print the coherence score\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95405059-6981-4951-be10-415bae02f1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot topics\n",
    "\n",
    "def plot_top_words_gensim(lda_model, gensim_dictionary, n_top_words, title):\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda_model.get_topics()):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [gensim_dictionary[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx + 1}', fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "    \n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    fig.suptitle(title, fontsize=40)\n",
    "    plt.show()\n",
    "    \n",
    "# Plot the top words from each topic\n",
    "plot_top_words_gensim(lda_model, gensim_dictionary, 10, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae00f9-2ae3-4e75-a367-676b793af798",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensimvis.prepare(lda_model, gensim_corpus, gensim_dictionary,mds='tsne')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857c214-f044-48a7-8a2a-79b9a6778161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25725b-d4e3-4cf8-a204-49f8a839f125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sensitivity analysis with respect to topics\n",
    "def compute_coherence_values(dictionary, corpus, texts, start, limit, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    start : Minimum number of topics to test\n",
    "    limit : Maximum number of topics to test\n",
    "    step : Step size for the number of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA models with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=gensim_corpus, \n",
    "                         id2word=gensim_dictionary,\n",
    "                         random_state=100,\n",
    "                         alpha = 0.01,\n",
    "                         eta = 0.91,\n",
    "                         num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def plot_coherence_sensitivity(start, limit, step, coherence_values):\n",
    "    \"\"\"\n",
    "    Plot coherence scores against the number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    start : Minimum number of topics to test\n",
    "    limit : Maximum number of topics to test\n",
    "    step : Step size for the number of topics\n",
    "    coherence_values : Coherence values corresponding to the LDA models with respective number of topics\n",
    "    \"\"\"\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.title(\"Coherence Score vs Number of Topics\")\n",
    "    plt.xticks(x)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have the variables 'gensim_dictionary', 'gensim_corpus', and 'texts' already set up:\n",
    "\n",
    "# Parameters for the sensitivity analysis\n",
    "start = 1\n",
    "limit = 11\n",
    "step = 1\n",
    "\n",
    "# Run the coherence value computation\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=gensim_dictionary, corpus=gensim_corpus, texts=tokenized_docs, start=start, limit=limit, step=step)\n",
    "\n",
    "# Plot the coherence score sensitivity\n",
    "plot_coherence_sensitivity(start, limit, step, coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35043d-3ded-4f63-90c9-64238838519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_model = model_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3de17-653f-413a-bdb5-36a16d557fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top words from each topic\n",
    "plot_top_words_gensim(selected_model, gensim_dictionary, 10, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed425f1c-2572-48e1-833a-5263810129c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LDAvis_evaluate = gensimvis.prepare(selected_model, gensim_corpus, gensim_dictionary,mds='tsne')\n",
    "LDAvis_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20c83d-df6f-4ef5-904e-bbaa6f7bb780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def explore_alpha_eta(dictionary, corpus, texts, num_topics, alpha_values, eta_values):\n",
    "    \"\"\"\n",
    "    Explore LDA models with different values of alpha and eta while fixing the number of topics.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    num_topics : Fixed number of topics\n",
    "    alpha_values : List of alpha values to explore\n",
    "    eta_values : List of eta values to explore\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    results : Dictionary with keys as tuples of (alpha, eta) and values as lists containing the model and coherence value\n",
    "    best_model : The model with the highest coherence score\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    best_coherence = 0.0\n",
    "    best_model = None\n",
    "    best_params = (None, None)\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        for eta in eta_values:\n",
    "            # Train LDA model\n",
    "            model = LdaModel(corpus=corpus, \n",
    "                             id2word=dictionary, \n",
    "                             num_topics=num_topics, \n",
    "                             alpha=alpha, \n",
    "                             eta=eta, \n",
    "                             random_state=100)\n",
    "            \n",
    "            # Compute coherence score\n",
    "            coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_score = coherence_model.get_coherence()\n",
    "            \n",
    "            # Update results\n",
    "            results[(alpha, eta)] = [model, coherence_score]\n",
    "            \n",
    "            # Update best model\n",
    "            if coherence_score > best_coherence:\n",
    "                best_coherence = coherence_score\n",
    "                best_model = model\n",
    "                best_params = (alpha, eta)\n",
    "    \n",
    "    return results, best_model, best_params\n",
    "\n",
    "# Example usage:\n",
    "alpha_values = list(np.arange(0.01,1,0.1))\n",
    "alpha_values.extend(['symmetric', 'asymmetric'])\n",
    "\n",
    "eta_values = list(np.arange(0.01,1,0.1))\n",
    "eta_values.extend(['symmetric'])\n",
    "num_topics = 10  # Set this to the best number of topics found from your previous sensitivity analysis\n",
    "\n",
    "# Run the function\n",
    "results, best_model, best_params = explore_alpha_eta(gensim_dictionary, gensim_corpus, tokenized_docs, num_topics, alpha_values, eta_values)\n",
    "\n",
    "# Display the best model\n",
    "print(f\"Best Model's Coherence Score: {results[best_params][1]}\")\n",
    "print(f\"Best Model's Alpha: {best_params[0]}\")\n",
    "print(f\"Best Model's Eta: {best_params[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3246590-8bef-4067-96cc-f5709a6eb652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00d637-a69a-4210-9fa7-e2cef5ec255c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize BERTopic model\n",
    "topic_model = BERTopic(nr_topics=10)\n",
    "\n",
    "# Fit the model to your data\n",
    "topics, probabilities = topic_model.fit_transform(data_normalized)\n",
    "\n",
    "# Explore the topics\n",
    "for topic in topic_model.get_topic_info().head():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f69e4-1abf-4e42-a1c1-e6a7a69afaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7b2e8-6c89-40ac-92cd-3185c9504b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a706bf-2580-404b-bcb4-72ceff55ea43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9b404-5719-4696-a051-8cc080433bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11213591-9bc7-4fa0-b8c4-7bfc34e5f009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ca969-d1fd-45df-87f2-565b072ca606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience] *",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
