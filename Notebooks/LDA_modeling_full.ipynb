{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8e173-5880-4be7-bc60-2686cddfca42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re #regular expression\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis, pyLDAvis.lda_model\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "import joblib\n",
    "\n",
    "from custom_package.text_processing import normalize_text, tokenizer_func, remove_emojis\n",
    "from custom_package.modeling import GensimLdaTransformer, get_topic_assignment\n",
    "from custom_package.modeling import topic_mapping_sk_lda, topic_mapping_gensim_lda\n",
    "from custom_package.database import get_raw_tweets, store_processed_tweets,get_training_raw_tweets\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302074d3-3943-4c39-91ff-cf21025dc294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7022f-0ff1-4d54-9285-ec734bc2764b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb6ecf-5eef-4b10-a11b-d93ad5a0661d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4899b1f-1e85-4a23-8b15-656b6bd68a3a",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158aca4-3790-4116-b286-475d5c01f124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_limit = 484000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49807667-dc59-49a1-a51d-6dc3073aaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_tweets(query_limit = 100):\n",
    "    raw_tweets = get_training_raw_tweets(query_limit)\n",
    "    data = {'id' : [tweet.id for tweet in raw_tweets],\n",
    "        'text' : [remove_emojis(tweet.text) for tweet in raw_tweets],\n",
    "        'company_id' : [tweet.company_id for tweet in raw_tweets],\n",
    "        'date' : [tweet.date for tweet in raw_tweets]\n",
    "        }\n",
    "    filtered_df = pd.DataFrame(data)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee743e2b-caba-4fc9-bbad-74a7d2742f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw tweets for training\n",
    "filtered_df = get_filtered_tweets(query_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f2277-e5d9-4410-9a9c-68c0d8b0bce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573e418-8f38-4fa3-accb-4be3ec273d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a4dfa-adb7-42e3-8fb7-38fd939e527f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df['company_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248a76d-4cf8-4b26-80fc-e980dd38bd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def normalize_text(documents,\n",
    "                   min_token_len=1,\n",
    "                   irrelevant_pos=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Keyword arguments:\n",
    "    documents -- (np.array[str]) the list of documents to be preprocessed\n",
    "    min_token_len -- (int) min_token_length required\n",
    "    irrelevant_pos -- (list) a list of irrelevant pos tags\n",
    "\n",
    "    Returns: np.array[str] the normalized documents\n",
    "    \"\"\"\n",
    "    normalized_documents = []\n",
    "\n",
    "    for text in documents:\n",
    "        #print(text)\n",
    "        # Remove Emails\n",
    "        text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "        # Remove extra space characters\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Remove distracting characters\n",
    "        text = re.sub(r'''[\\*\\~]+''', \"\", text)\n",
    "\n",
    "        doc = nlp(text) #covert text into spacy object\n",
    "        clean_text = []\n",
    "\n",
    "        for token in doc:\n",
    "            if (token.is_stop == False # Check if it's not a stopword\n",
    "                and token.is_alpha # Check if it's an alphanumerics char\n",
    "                and len(token) > min_token_len # Check if the word meets minimum threshold\n",
    "                and token.pos_ not in irrelevant_pos): # Check if the POS is in the acceptable POS tags\n",
    "                lemma = token.lemma_ # Take the lemma of the word\n",
    "                clean_text.append(lemma)\n",
    "\n",
    "        clean_text = ' '.join(clean_text) #merge list of tokens back into string\n",
    "        normalized_documents.append(clean_text) #append to list of normalized documents\n",
    "\n",
    "    normalized_documents = np.array(normalized_documents) #convert list of normalized documents into numpy array\n",
    "    return normalized_documents\n",
    "\n",
    "# Create a Transformer from the function so that we can use it in a Pipeline\n",
    "normalizer = FunctionTransformer(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107ce26-8545-46c6-83f4-af48fcb6c9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the built-in stopword list\n",
    "builtin_stopwords = list(ENGLISH_STOP_WORDS)\n",
    "# Your custom stopwords\n",
    "custom_stopwords = ['una', 'dey', 'come','dm','pls','guy',\n",
    "                    'hi','try','hello','god','gtb','gtbank','nigeria','till','gt',\n",
    "                   'send','month','week','day','february','don','useless','want',\n",
    "                    'people','know','abeg']\n",
    "#new_stopwords = ['ment','uba','access','bad','beg','good','morning',\n",
    "#                 'yesterday','zenith','firstbank','new','use','youfirst','year']\n",
    "new_stopwords = ['customer','service','bank','ment',\n",
    "'uba','access','bad','beg','need','good','morning',\n",
    "'yesterday','zenith','firstbank','new','use',\n",
    "'youfirst','money','help','dear','ur','na','naira','think',\n",
    "'thank','person','tell','respond','like','wait','time','attend','say','treat','today',\n",
    "'ooo','thing','life','happen','happy','africa','business','start','win','way','year','hour','ask']\n",
    "# Combine the stopword lists\n",
    "all_stopwords = builtin_stopwords + custom_stopwords + new_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096e5f0-ea4a-4094-9841-9aac6dc6d8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d08a7-e4dd-4135-9afb-2f837cb956fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_features = 5000*2\n",
    "\n",
    "#keep 5000 most common tokens that appear in atleast 2 documents, less than 95% of documents\n",
    "#notice binary=False by default\n",
    "vectorizer = CountVectorizer(min_df=100, max_df=0.95, \n",
    "                             max_features=n_features,\n",
    "                             ngram_range=(1, 3),\n",
    "                             stop_words=all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df3ad0-cb89-4107-864b-df890b114fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([('normalizer', normalizer),\n",
    "                         ('vectorizer', vectorizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059998b7-c49f-437f-b755-676a0adfdaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "\n",
    "#alpha = doc_topic_prior = 1 / n_components (every topic is equally likely in a document)\n",
    "#eta = topic_word_prior = 1 / n_components (every word is equally likely in a topic)\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                max_iter=10,\n",
    "                                doc_topic_prior = 0.01,\n",
    "                                topic_word_prior = 0.01,\n",
    "                                learning_method='batch',\n",
    "                                random_state=27,\n",
    "                               verbose = 1,\n",
    "                               n_jobs=-1)\n",
    "\n",
    "pipeline = Pipeline([('preprocessor', preprocessor),\n",
    "                     ('model', lda)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be060a2-67e0-4c68-b594-c711bc06d462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.fit(filtered_df['text'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc031dfa-7fa7-4970-84cf-01a356598349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df['text'].iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadebfeb-92ee-4327-a233-c307e7d839e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[print(max(range(len(topic)), key=topic.__getitem__)) for topic in pipeline.transform(filtered_df['text'].iloc[0:10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e81a8-3a45-4e0f-9d11-de63a9a28b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a4528-bdad-4940-83c9-7b5316266918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fd1af-e8c9-4be2-928d-79875995305e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "plot_top_words(lda, vectorizer.get_feature_names_out(), 10, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9f81f-4dc3-4001-8623-1e7bfb3c0aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_vectorized = preprocessor.transform(filtered_df['text'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920c616-a4c4-49ca-b571-9e3ca96d75a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis = pyLDAvis.lda_model.prepare(lda, data_vectorized, vectorizer, mds='pcoa',sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a42d2-642c-4420-95c9-ffb72e6f4aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # Get top words indices for the topic\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        # Get the actual words\n",
    "        top_words[f'Topic {topic_idx + 1}'] = [feature_names[i] for i in top_features_ind]\n",
    "    return top_words\n",
    "\n",
    "def create_word_clouds(top_words):\n",
    "    for topic, words in top_words.items():\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Word Cloud for {topic}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "# lda is your trained LDA model\n",
    "# feature_names are obtained from your vectorizer, e.g., vectorizer.get_feature_names_out()\n",
    "top_words = get_top_words(lda, vectorizer.get_feature_names_out(), 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fdb00-9c00-4529-8b94-c15bd4a5c8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#joblib.dump(top_words,'top_words.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ad31c-31f4-4f96-8cd2-1bb5c17a89c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_word_clouds(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acf202-8613-4374-9907-05e442fd5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(pipeline,'full_lda_pipeline.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience] *",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
